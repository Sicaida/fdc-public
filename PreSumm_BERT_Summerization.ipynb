{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PreSumm BERT Summerization",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sicaida/fdc-public/blob/master/PreSumm_BERT_Summerization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eYNy_EzFi27",
        "colab_type": "text"
      },
      "source": [
        "Just the absolute minimum required to get this working at all. Most of the work was done in this https://github.com/mingchen62/PreSumm.git fork.\n",
        "\n",
        "Python 3\n",
        "Runtime GPU? (Actually GPU doesn't work?)\n",
        "\n",
        "Also see: https://github.com/nlpyang/PreSumm/issues/11\n",
        "\n",
        "txt files go in /content/PreSumm/bert_data/cnndm\n",
        "\n",
        "Just try one file at once, I didn't bother making it really work with multiple files.\n",
        "\n",
        "results show up in /content/PreSumm/results and also printed in the cell.\n",
        "\n",
        "Quick sample output with fiction: https://twitter.com/jonathanfly/status/1168377996597764096"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGAVMKjfuWFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag3GRLiOFV2R",
        "colab_type": "code",
        "outputId": "ba2c77e5-1564-4480-f4e8-8be131beddc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone https://github.com/mingchen62/PreSumm.git #based on this fork which handles the encoding etc\n",
        "%cd PreSumm\n",
        "!pip install pytorch_transformers tensorboardX pyrouge\n",
        "%cd /content/PreSumm/models\n",
        "!gdown https://drive.google.com/uc?id=1kKWoV0QCbeIuFt85beQgJ4v0lujaXobJ&export=download #CNN/DM Extractive bertext_cnndm_transformer.pt\n",
        "!gdown https://drive.google.com/uc?id=1-IKVCtc4Q-BdZpjXc4s70_fRsWnjtYLr&export=download #CNN/DM Abstractive model_step_148000.pt \n",
        "!gdown https://drive.google.com/uc?id=1H50fClyTkNprWJNh10HWdGEdDdQIkzsI&export=download #XSUM (One Sentence Summary) model_step_30000.pt   \n",
        "!unzip /content/PreSumm/models/bertext_cnndm_transformer.zip\n",
        "!unzip /content/PreSumm/models/bertsumextabs_cnndm_final_model.zip\n",
        "!unzip /content/PreSumm/models/bertsumextabs_xsum_final_model.zip\n",
        "!mkdir /content/PreSumm/models/CNN_DailyMail_Extractive\n",
        "!mkdir /content/PreSumm/models/CNN_DailyMail_Abstractive\n",
        "!mkdir /content/PreSumm/models/XSUM_OneSentence\n",
        "!mv /content/PreSumm/models/bertext_cnndm_transformer.pt /content/PreSumm/models/CNN_DailyMail_Extractive\n",
        "!mv /content/PreSumm/models/model_step_148000.pt /content/PreSumm/models/CNN_DailyMail_Abstractive\n",
        "!mv /content/PreSumm/models/model_step_30000.pt /content/PreSumm/models/XSUM_OneSentence"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PreSumm'...\n",
            "remote: Enumerating objects: 154, done.\u001b[K\n",
            "remote: Total 154 (delta 0), reused 0 (delta 0), pack-reused 154\u001b[K\n",
            "Receiving objects: 100% (154/154), 12.97 MiB | 17.97 MiB/s, done.\n",
            "Resolving deltas: 100% (64/64), done.\n",
            "/content/PreSumm\n",
            "Collecting pytorch_transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 3.5MB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 49.7MB/s \n",
            "\u001b[?25hCollecting pyrouge\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/85/e522dd6b36880ca19dcf7f262b22365748f56edc6f455e7b6a37d0382c32/pyrouge-0.1.3.tar.gz (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 25.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (4.28.1)\n",
            "Collecting sentencepiece (from pytorch_transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 46.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2.21.0)\n",
            "Collecting regex (from pytorch_transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.16.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.9.224)\n",
            "Collecting sacremoses (from pytorch_transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/04/b92425ca552116afdb7698fa3f00ca1c975cfd86a847cf132fd813c5d901/sacremoses-0.0.34.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 37.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2019.6.16)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.224 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (1.12.224)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (0.13.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (41.2.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.224->boto3->pytorch_transformers) (2.5.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.224->boto3->pytorch_transformers) (0.15.2)\n",
            "Building wheels for collected packages: pyrouge, regex, sacremoses\n",
            "  Building wheel for pyrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrouge: filename=pyrouge-0.1.3-cp36-none-any.whl size=191613 sha256=9f43f16ea6a02380c567e669a8f3f1aca214f3e2bc434e0ae9d68169ca3c1a91\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/d3/0c/e5b04e15b6b87c42e980de3931d2686e14d36e045058983599\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.8.19-cp36-cp36m-linux_x86_64.whl size=609227 sha256=91d03e720ed4f734a4096e64a4ff7b311c6b1346d6083fcd0b520e4534e09149\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.34-cp36-none-any.whl size=883992 sha256=556c8c0b3a105d0d7944f5dd5ab22763b0ce847b5fdc04e122961b00c7833e1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/b9/5b/8bd674c23e962fbff34420a9fa7a2c374d591ecadd5bc37684\n",
            "Successfully built pyrouge regex sacremoses\n",
            "Installing collected packages: sentencepiece, regex, sacremoses, pytorch-transformers, tensorboardX, pyrouge\n",
            "Successfully installed pyrouge-0.1.3 pytorch-transformers-1.2.0 regex-2019.8.19 sacremoses-0.0.34 sentencepiece-0.1.83 tensorboardX-1.8\n",
            "/content/PreSumm/models\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kKWoV0QCbeIuFt85beQgJ4v0lujaXobJ\n",
            "To: /content/PreSumm/models/bertext_cnndm_transformer.zip\n",
            "1.32GB [00:09, 134MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-IKVCtc4Q-BdZpjXc4s70_fRsWnjtYLr\n",
            "To: /content/PreSumm/models/bertsumextabs_cnndm_final_model.zip\n",
            "1.98GB [00:14, 139MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1H50fClyTkNprWJNh10HWdGEdDdQIkzsI\n",
            "To: /content/PreSumm/models/bertsumextabs_xsum_final_model.zip\n",
            "1.98GB [00:16, 120MB/s]\n",
            "Archive:  /content/PreSumm/models/bertext_cnndm_transformer.zip\n",
            "  inflating: bertext_cnndm_transformer.pt  \n",
            "Archive:  /content/PreSumm/models/bertsumextabs_cnndm_final_model.zip\n",
            "  inflating: model_step_148000.pt    \n",
            "Archive:  /content/PreSumm/models/bertsumextabs_xsum_final_model.zip\n",
            "  inflating: model_step_30000.pt     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTHoYeb5F-zb",
        "colab_type": "code",
        "outputId": "3beb59df-8a44-480c-d7b6-306e10330c3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!mkdir /content/PreSumm/bert_data_test/\n",
        "!mkdir /content/PreSumm/bert_data/cnndm\n",
        "%cd /content/PreSumm/bert_data/cnndm\n",
        "#!wget https://pastebin.com/raw/L96bGaXL #first chapter of Game of Thrones\n",
        "#!wget https://pastebin.com/raw/0CW5z6fp #first chapter Harry Potter"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/PreSumm/bert_data/cnndm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx9xcgztGFc1",
        "colab_type": "code",
        "outputId": "b8307c19-521a-4bfa-b1d3-e5d33a6b08e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHbfrhheGHKF",
        "colab_type": "code",
        "outputId": "96a853dc-2b8d-4a0e-9e41-0f5a5e9e55e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile /content/PreSumm/src/summarizer.py\n",
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "    Main training workflow\n",
        "\"\"\"\n",
        "from __future__ import division\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "from others.logging import init_logger\n",
        "from train_abstractive import validate_abs, train_abs, baseline, test_abs, test_text_abs, load_models_abs\n",
        "from train_extractive import train_ext, validate_ext, test_ext\n",
        "from prepro import data_builder\n",
        "import glob, os\n",
        "\n",
        "model_flags = ['hidden_size', 'ff_size', 'heads', 'emb_size', 'enc_layers', 'enc_hidden_size', 'enc_ff_size',\n",
        "               'dec_layers', 'dec_hidden_size', 'dec_ff_size', 'encoder', 'ff_actv', 'use_interval']\n",
        "\n",
        "\n",
        "def str2bool(v):\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
        "\n",
        "\n",
        "\n",
        "def init_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"-task\", default='abs', type=str, choices=['ext', 'abs'])\n",
        "    parser.add_argument(\"-encoder\", default='bert', type=str, choices=['bert', 'baseline'])\n",
        "    parser.add_argument(\"-mode\", default='test', type=str, choices=['train', 'validate', 'test'])\n",
        "    parser.add_argument(\"-bert_data_path\", default='../../bert_data_new/cnndm')\n",
        "    parser.add_argument(\"-model_path\", default='../../models/')\n",
        "    parser.add_argument(\"-result_path\", default='../../results/cnndm')\n",
        "    parser.add_argument(\"-temp_dir\", default='../../temp')\n",
        "\n",
        "    parser.add_argument(\"-batch_size\", default=140, type=int)\n",
        "    parser.add_argument(\"-test_batch_size\", default=200, type=int)\n",
        "\n",
        "    parser.add_argument(\"-max_pos\", default=800, type=int)\n",
        "    parser.add_argument(\"-use_interval\", type=str2bool, nargs='?',const=True,default=True)\n",
        "    parser.add_argument(\"-large\", type=str2bool, nargs='?',const=True,default=False)\n",
        "    parser.add_argument(\"-load_from_extractive\", default='', type=str)\n",
        "\n",
        "    parser.add_argument(\"-sep_optim\", type=str2bool, nargs='?',const=True,default=True)\n",
        "    parser.add_argument(\"-lr_bert\", default=2e-3, type=float)\n",
        "    parser.add_argument(\"-lr_dec\", default=2e-3, type=float)\n",
        "    parser.add_argument(\"-use_bert_emb\", type=str2bool, nargs='?',const=True,default=False)\n",
        "\n",
        "    parser.add_argument(\"-share_emb\", type=str2bool, nargs='?', const=True, default=False)\n",
        "    parser.add_argument(\"-finetune_bert\", type=str2bool, nargs='?', const=True, default=True)\n",
        "    parser.add_argument(\"-dec_dropout\", default=0.2, type=float)\n",
        "    parser.add_argument(\"-dec_layers\", default=6, type=int)\n",
        "    parser.add_argument(\"-dec_hidden_size\", default=768, type=int)\n",
        "    parser.add_argument(\"-dec_heads\", default=8, type=int)\n",
        "    parser.add_argument(\"-dec_ff_size\", default=2048, type=int)\n",
        "    parser.add_argument(\"-enc_hidden_size\", default=512, type=int)\n",
        "    parser.add_argument(\"-enc_ff_size\", default=512, type=int)\n",
        "    parser.add_argument(\"-enc_dropout\", default=0.2, type=float)\n",
        "    parser.add_argument(\"-enc_layers\", default=6, type=int)\n",
        "\n",
        "    # params for EXT\n",
        "    parser.add_argument(\"-ext_dropout\", default=0.2, type=float)\n",
        "    parser.add_argument(\"-ext_layers\", default=2, type=int)\n",
        "    parser.add_argument(\"-ext_hidden_size\", default=768, type=int)\n",
        "    parser.add_argument(\"-ext_heads\", default=8, type=int)\n",
        "    parser.add_argument(\"-ext_ff_size\", default=2048, type=int)\n",
        "\n",
        "    parser.add_argument(\"-label_smoothing\", default=0.1, type=float)\n",
        "    parser.add_argument(\"-generator_shard_size\", default=32, type=int)\n",
        "    parser.add_argument(\"-alpha\",  default=0.6, type=float)\n",
        "    parser.add_argument(\"-beam_size\", default=5, type=int)\n",
        "    parser.add_argument(\"-min_length\", default=15, type=int)\n",
        "    parser.add_argument(\"-max_length\", default=150, type=int)\n",
        "    parser.add_argument(\"-max_tgt_len\", default=140, type=int)\n",
        "\n",
        "    # params for preprocessing\n",
        "    parser.add_argument(\"-shard_size\", default=2000, type=int)\n",
        "    parser.add_argument('-min_src_nsents', default=3, type=int)\n",
        "    parser.add_argument('-max_src_nsents', default=100, type=int)\n",
        "    parser.add_argument('-min_src_ntokens_per_sent', default=5, type=int)\n",
        "    parser.add_argument('-max_src_ntokens_per_sent', default=200, type=int)\n",
        "    parser.add_argument('-min_tgt_ntokens', default=5, type=int)\n",
        "    parser.add_argument('-max_tgt_ntokens', default=500, type=int)\n",
        "    parser.add_argument(\"-lower\", type=str2bool, nargs='?',const=True,default=True)\n",
        "    parser.add_argument(\"-use_bert_basic_tokenizer\", type=str2bool, nargs='?',const=True,default=False)\n",
        "\n",
        " \n",
        "    parser.add_argument(\"-param_init\", default=0, type=float)\n",
        "    parser.add_argument(\"-param_init_glorot\", type=str2bool, nargs='?',const=True,default=True)\n",
        "    parser.add_argument(\"-optim\", default='adam', type=str)\n",
        "    parser.add_argument(\"-lr\", default=1, type=float)\n",
        "    parser.add_argument(\"-beta1\", default= 0.9, type=float)\n",
        "    parser.add_argument(\"-beta2\", default=0.999, type=float)\n",
        "    parser.add_argument(\"-warmup_steps\", default=8000, type=int)\n",
        "    parser.add_argument(\"-warmup_steps_bert\", default=8000, type=int)\n",
        "    parser.add_argument(\"-warmup_steps_dec\", default=8000, type=int)\n",
        "    parser.add_argument(\"-max_grad_norm\", default=0, type=float)\n",
        "\n",
        "    parser.add_argument(\"-save_checkpoint_steps\", default=5, type=int)\n",
        "    parser.add_argument(\"-accum_count\", default=1, type=int)\n",
        "    parser.add_argument(\"-report_every\", default=1, type=int)\n",
        "    parser.add_argument(\"-train_steps\", default=1000, type=int)\n",
        "    parser.add_argument(\"-recall_eval\", type=str2bool, nargs='?',const=True,default=False)\n",
        "\n",
        "\n",
        "    parser.add_argument('-visible_gpus', default='-1', type=str)\n",
        "    parser.add_argument('-gpu_ranks', default='0', type=str)\n",
        "    parser.add_argument('-log_file', default='../../logs/cnndm.log')\n",
        "    parser.add_argument('-seed', default=666, type=int)\n",
        "\n",
        "    parser.add_argument(\"-test_all\", type=str2bool, nargs='?',const=True,default=False)\n",
        "    parser.add_argument(\"-test_from\", default='')\n",
        "    parser.add_argument(\"-test_start_from\", default=-1, type=int)\n",
        "\n",
        "    parser.add_argument(\"-train_from\", default='')\n",
        "    parser.add_argument(\"-report_rouge\", type=str2bool, nargs='?',const=True,default=True)\n",
        "    parser.add_argument(\"-block_trigram\", type=str2bool, nargs='?', const=True, default=True)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    args.gpu_ranks = [int(i) for i in range(len(args.visible_gpus.split(',')))]\n",
        "    args.world_size = len(args.gpu_ranks)\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus\n",
        "\n",
        "    init_logger(args.log_file)\n",
        "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
        "    device_id = 0 if device == \"cuda\" else -1\n",
        "\n",
        "    return args, device_id\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args, device_id = init_args()\n",
        "    print(args.task, args.mode) \n",
        "\n",
        "    cp = args.test_from\n",
        "    try:\n",
        "    \tstep = int(cp.split('.')[-2].split('_')[-1])\n",
        "    except:\n",
        "    \tstep = 0\n",
        "\n",
        "    predictor = load_models_abs(args, device_id, cp, step)\n",
        "\n",
        "    all_files = glob.glob(os.path.join('/content/drive/My Drive/summary', '*'))\n",
        "    #print('Files In Input Dir: ' + str(len(all_files)))\n",
        "    for i, file in enumerate(all_files):\n",
        "        with open(file) as f:\n",
        "            source=f.read().rstrip()\n",
        "\n",
        "        data_builder.str_format_to_bert(  source, args, '../bert_data_test/cnndm.test.0.bert.pt') \n",
        "        args.result_path= '../results/content'+str(i)\n",
        "        args.bert_data_path= '../bert_data_test/cnndm'\n",
        "        test_text_abs(args, device_id, cp, step, predictor)\n",
        "        \n",
        "        tgt, time_used = test_text_abs(args, device_id, cp, step, predictor)\n",
        "\n",
        "        # some postprocessing \n",
        "\n",
        "        sentences = tgt.split('<q>')\n",
        "        sentences = [sent.capitalize() for sent in sentences]\n",
        "        sentences = '. '.join(sentences).rstrip()\n",
        "        sentences = sentences.replace(' ,', ',')\n",
        "        sentences = sentences+'.'\n",
        "\n",
        "        print(\"summary [{}]\".format(sentences))\n",
        "        print(\"time used {}\".format(time_used))\n",
        "    \n",
        "  \n",
        "#   EXTRACT FROM JSON FILE\n",
        "#   data2 = {}   \n",
        "#     with open('/content/drive/My Drive/summary/input.json') as json_file:\n",
        "#       data2 = json.load(json_file)\n",
        "#       for p in data2:\n",
        "#         source = data2[p]['FIELD2']\n",
        "#         data_builder.str_format_to_bert(  source, args, '../bert_data_test/cnndm.test.0.bert.pt') \n",
        "#         args.result_path= '../results/content' #+str(i)\n",
        "#         args.bert_data_path= '../bert_data_test/cnndm'\n",
        "#         test_text_abs(args, device_id, cp, step, predictor)\n",
        "\n",
        "#         tgt, time_used = test_text_abs(args, device_id, cp, step, predictor)\n",
        "\n",
        "#         # some postprocessing \n",
        "\n",
        "#         sentences = tgt.split('<q>')\n",
        "#         sentences = [sent.capitalize() for sent in sentences]\n",
        "#         sentences = '. '.join(sentences).rstrip()\n",
        "#         sentences = sentences.replace(' ,', ',')\n",
        "#         sentences = sentences+'.'\n",
        "\n",
        "#         print(\"summary [{}]\".format(sentences))\n",
        "#         print(\"time used {}\".format(time_used))\n",
        "        \n",
        "#         data2[p]['FIELD2'] = sentences\n",
        "\n",
        "#     with open('/content/drive/My Drive/summary/results.json', 'w') as outfile:\n",
        "#       json.dump(data2, outfile, indent=4)\n",
        "\n",
        "\n",
        "# EXTRACT FROM CSV\n",
        "# import csv\n",
        "# with open('/content/drive/My Drive/summary/input.csv') as csv_file, open('/content/drive/My Drive/summary/outputExt.csv', 'w') as output:\n",
        "#     csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "#     writer = csv.writer(output, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    \n",
        "#     for row in csv_reader:      \n",
        "        \n",
        "        \n",
        "#         source = row[1]\n",
        "#         data_builder.str_format_to_bert(  source, args, '../bert_data_test/cnndm.test.0.bert.pt') \n",
        "#         args.result_path= '../results/content' #+str(i)\n",
        "#         args.bert_data_path= '../bert_data_test/cnndm'\n",
        "#         test_text_abs(args, device_id, cp, step, predictor)\n",
        "\n",
        "#         tgt, time_used = test_text_abs(args, device_id, cp, step, predictor)\n",
        "\n",
        "\n",
        "#         sentences = tgt.split('<q>')\n",
        "#         sentences = [sent.capitalize() for sent in sentences]\n",
        "#         sentences = '. '.join(sentences).rstrip()\n",
        "#         sentences = sentences.replace(' ,', ',')\n",
        "#         sentences = sentences+'.'\n",
        "\n",
        "        \n",
        "#         row[1] = sentences\n",
        "        \n",
        "#         new_row = [row[0], row[1]]\n",
        "#         print(new_row)\n",
        "#         writer.writerow(new_row)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/PreSumm/src/summarizer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJCquU_Peul3",
        "colab_type": "code",
        "outputId": "f094e6e0-556f-478d-c7ff-062ccca810fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#CNN_DM abstractive\n",
        "%cd /content/PreSumm/src\n",
        "!python summarizer.py -task abs -mode test -test_from /content/PreSumm/models/CNN_DailyMail_Abstractive/model_step_148000.pt -batch_size 32 -test_batch_size 500 -bert_data_path .../drive/My%20Drive/summary -log_file ../logs/val_abs_bert_cnndm -report_rouge False  -sep_optim true -use_interval true -visible_gpus 0,1,2 -max_pos 512 -max_src_nsents 100 -max_length 200 -alpha 0.95 -min_length 50 "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/PreSumm/src\n",
            "abs test\n",
            "[2019-09-25 07:15:55,397 INFO] Loading checkpoint from /content/PreSumm/models/CNN_DailyMail_Abstractive/model_step_148000.pt\n",
            "Namespace(accum_count=1, alpha=0.95, batch_size=32, beam_size=5, bert_data_path='.../drive/My%20Drive/summary', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/val_abs_bert_cnndm', lower=True, lr=1, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=500, max_pos=512, max_src_nsents=100, max_src_ntokens_per_sent=200, max_tgt_len=140, max_tgt_ntokens=500, min_length=200, min_src_nsents=3, min_src_ntokens_per_sent=5, min_tgt_ntokens=5, mode='test', model_path='../../models/', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=False, result_path='../../results/cnndm', save_checkpoint_steps=5, seed=666, sep_optim=True, shard_size=2000, share_emb=False, task='abs', temp_dir='../../temp', test_all=False, test_batch_size=500, test_from='/content/PreSumm/models/CNN_DailyMail_Abstractive/model_step_148000.pt', test_start_from=-1, train_from='', train_steps=1000, use_bert_basic_tokenizer=False, use_bert_emb=False, use_interval=True, visible_gpus='0,1,2', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=3)\n",
            "[2019-09-25 07:15:57,293 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
            "[2019-09-25 07:15:57,294 INFO] Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[2019-09-25 07:15:57,488 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "[2019-09-25 07:16:05,962 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "[2019-09-25 07:16:06,325 INFO] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "[2019-09-25 07:16:06,376 INFO] Processing                                 FOR CHILDREN:\n",
            "\n",
            "                                   Sly Fox\n",
            "\n",
            "    Mr. Rabbit sat on his front porch rocking, eating a great big carrot, \n",
            "and looking.\n",
            "\n",
            "    \"Looks like Sly Fox coming down the road,\" he said to himself, walking \n",
            "to the end of the porch. Shading his eyes with his paws, he exclaimed, \"It \n",
            "is Sly Fox.\" \n",
            "\n",
            "    \"Good morning Mr. Rabbit,\" cried Sly Fox, as he walked across the yard. \n",
            "\"Good morning,\" replied Mr. Rabbit, a slight frown on his face. \n",
            "\n",
            "    \"Well,\" said Sly Fox, \"as I haven't seen you in so long a time, thought \n",
            "I would stop and chat a while.\" \n",
            "\n",
            "    Mr. Rabbit could not be rude in his own home, even to an enemy, so he \n",
            "offered Sly Fox a seat on the porch. \n",
            "\n",
            "    \"Take a chair,\" he said politely. \tBut Sly Fox did not stay long, and as \n",
            "he was leaving, he asked: \"Mr. Rabbit, my mother is having a good dinner \n",
            "tonight. Won't you, Mrs. Rabbit, and your three little rabs come to dinner \n",
            "with me?\" \n",
            "\n",
            "    Oh, thought Mr. Rabbit, he knows about my little rabs and wants to take \n",
            "us off to eat us. He pretended to be disappointed as he replied: \"Sorry, Sly \n",
            "Fox, we have an engagement for today, but if you want us we can come \n",
            "tomorrow.\" \n",
            "\n",
            "    At this Sly Fox chuckled inwardly, and readily agreed to come for them \n",
            "the next day. Wishing Mr. Rabbit \"Good day\", he trotted on down the road \n",
            "toward his home. \n",
            "\n",
            "    As soon as he was out of sight, Mr. Rabbit ran into his house and called \n",
            "Mrs. Rabbit. \"Get all our things together,\" he said, \"and put rubber boots \n",
            "on our little rabs. We have to move quickly to the Piney Woods across the \n",
            "brook. Old Sly Fox has found our home and will destroy us.\" \n",
            "\n",
            "    In no time at all the Rabbit family had moved, and the little rabs were \n",
            "delighted with their new home. A woodland of towering pines it was, the \n",
            "ground covered with pine needles which made a soft carpeting. The wind made \n",
            "music in the pine trees, birds sang, and the fragrance of flowers filled the \n",
            "air. They found a huge hollow tree where Mr. Rabbit burrowed deep and made \n",
            "them a cozy home. Squirrels had left nuts hidden around in the old tree. \n",
            "Owls hooted throughout the night, crickets chirped merrily. \n",
            "\n",
            "    Next morning old Sly Fox knocked on the door where he had left Mr. \n",
            "Rabbit. Mrs. Hedgehog answered the door. \"Good morning, Mrs. Hedgehog. Is \n",
            "Mr. Rabbit in?\" inquired Sly Fox with a wicked grin and a cunning look in \n",
            "his eyes. \n",
            "\n",
            "    \"No,\" replied Mrs. Eedgehog, none too cordially. \"The Rabbit family \n",
            "moved to parts unknown right after you left yesterday.\" \n",
            "\n",
            "    \"Ah,\" exclaimed old Sly Fox, \"Mr. Rabbit and family were going to have \n",
            "dinner with me. My mother has planned a real feast. Why don't you come and \n",
            "enjoy it with us?\" \n",
            "\n",
            "    \"Oh,\" replied Mrs. Hedgehog, smacking her lips and thinking of all the \n",
            "goodies, \"I have just moved in and there is so much to do! Why not let it go \n",
            "until tomorrow?\" \n",
            "\n",
            "    \"Do you like nice young grasshoppers?\" asked Sly Fox softly. \n",
            "\n",
            "    \"Do I? Nothing so good as tender young grasshoppers,\" answered Mrs. \n",
            "Hedgehog, fairly dribbling at the mouth at the thought of such a dainty. \n",
            "\n",
            "    \"Well,\" said Sly Fox, \"we pass a field where there are any number of \n",
            "them. Come get in this sack, and when I stop in the field we will open the \n",
            "sack and rake in all of them we want. Mother will bake them with apples and \n",
            "they will be deilicious!\" This was too much for greedy Mrs. Hedgehog to \n",
            "resist, so in the sack she went. Sly Fox with a grin grabbed the sack, threw \n",
            "it over his shoulder and trotted toward home. \tAfter going a long way, Mrs. \n",
            "Hedgehog became suspicious and cried, \"How long before we reach that field \n",
            "of grasshoppers?\" \n",
            "\n",
            "    \"Why, you silly, greedy hedgehog, there is no field of grasshoppers for \n",
            "you. I am going to eat you for my dinner. It's you with apple dumplings that \n",
            "my mother will bake.\" \n",
            "\n",
            "    Every hair on Mrs. Hedgehog's head stood on end with fright. Oh, how \n",
            "foolish she had been! Her greed had trapped her. If only she had stayed home \n",
            "and straightened her house and cooked her own dinner, she would not have \n",
            "been in this sack to be eaten by Sly Fox. Greediness never pays, she thought \n",
            "to herself. \n",
            "\n",
            "    Sly Fox became tired, and as a slight rain had begun to fall, he looked \n",
            "for a dry place to sit down. Throwing the sack to the ground and chuckling \n",
            "at the thought of sitting on Mrs. Hedgehog, he dropped heavily upon the \n",
            "sack. \n",
            "\n",
            "    \"Wow, Wow!\" he cried, jumping quickly up, for Mrs. Hedgehog shot her \n",
            "sharp quills into him with all her might. \n",
            "\n",
            "    Sly Fox ran to and fro trying to pull out the quills, but they had gone \n",
            "too deep. Home he ran, screaming to his mother. Old Mother Fox threw him \n",
            "over a log and began pulling out the quills, at the same time calling to a \n",
            "neighbor fox to bring some honey to put on the places where the quills had \n",
            "been. \n",
            "\n",
            "    Mrs. Hedgehog crawled out of the bag and began walking slowly toward \n",
            "home. She thought to herself that never again would she be so greedy and \n",
            "allow herself to be fooled by Sly Fox or any one else. \n",
            "\n",
            "    Meanwhile, Mr. Rabbit and family were living happily in Piney Woods. The \n",
            "little rabs played on the crystal clear brook that ran through the woods, \n",
            "wading, sailing little leaf boats, and trying to catch the silvery minnows \n",
            "darting here and there. \n",
            "\n",
            "    Late one evening Papa and Mama Rabbit were sitting before the cozy fire \n",
            "talking. Papa Rabbit had on his house robe and bedroom slippers, reading the \n",
            "newspaper. Every now and then he looked over his spectacles lovingly at \n",
            "dainty little Mama Rabbit, dressed in a flowered housecoat and red slippers \n",
            "and knitting little socks for the little rabs. \n",
            "\n",
            "    \"Sniff! Sniff! Sniff!\" came suddenly to their ears. \n",
            "\n",
            "    \"Sly Fox!\" whispered Papa Rabbit, his face now full of concern and \n",
            "alarm. \n",
            "\n",
            "    \"Yes,\" agreed Mama Rabbit, her voice trembling with fright. \n",
            "\n",
            "    \"Go cover the little rabs with straw and tell them to be very, very \n",
            "quiet,\" instructed Papa Rabbit. \n",
            "\n",
            "    Mrs. Rabbit quickly covered the little rabs and cautioned them to be as \n",
            "quiet as mice. Since they were well behaved and obedient little rabs, they \n",
            "did just as their mother told them. \n",
            "\n",
            "    \"I left my big stick beside the old oak tree,\" cried Papa Rabbit under \n",
            "his breath. \"What shall we do?\" \n",
            "\n",
            "    \"Sniff! Sniff! Sniff!\" went Sly Fox again, scratching up the earth by \n",
            "the old hollow tree as he began to dig furiously. The poor little Rabbit \n",
            "family sat still and frightened, their hearts thumping, their paws shaking, \n",
            "and their eyes bulging with panic. Suddenly in the distance they heard the \n",
            "\"Toot! Toot!Toot!\" of horns, and the \"Woof! Woof! Woof!\" of barking dogs. \n",
            "\n",
            "    Papa Rabbit whispered, \"Fox hunters!\" as his heart gave a bound of \n",
            "relief. \n",
            "\n",
            "    Nearer and nearer came the baying of the hounds and the music of the \n",
            "horns. Old Sly Fox was so busily digging that he failed to hear at first, \n",
            "but suddenly he stopped digging, and threw back his ears to listen. Then he \n",
            "quickly jumped away from the log where the Rabbit family lived and started \n",
            "running. \n",
            "\n",
            "    But the hounds were right after him, baying loudly with all their might. \n",
            "The horses' feet beat out an excited rhythm as the red-coated fox hunters \n",
            "urged them on in the chase. Up hill, over the meadows they ran. \n",
            "\n",
            "    Sly Fox was now running for his life, but the dogs were getting closer \n",
            "and closer. He jumped across the brook and spied a hole among some bushes. \n",
            "Into this he slid, and as the dogs went down the side of the stream of water \n",
            "before they jumped across they lost his scent. Sly Fox quickly ran out of \n",
            "the hole and took off in the opposite direction from the way the dogs were \n",
            "going. He had been so frightened and so near death that he resolved to \n",
            "himself never to bother the Rabbit family again. \n",
            "\n",
            "    Meanwhile, when Papa Rabbit had heard the hounds start the chase, he \n",
            "turned to Mama Rabbit and cried, \"Safe at last! Call our little rabs for \n",
            "prayers of thanksgiving and praise to our Father which art in heaven.\" \n",
            "\n",
            "    After prayers, Mama Rabbit hustled about making mint tea for her and \n",
            "Papa Rabbit, and hot chocolate piled high with whipped cream for the little \n",
            "rabs. After that time they Lived happily among the great whispering pines, \n",
            "never bothered by old Sly Fox.\n",
            "                                                          --Beulah Murrelle\n",
            "[2019-09-25 07:16:06,420 INFO] Saving to ../bert_data_test/cnndm.test.0.bert.pt\n",
            "[2019-09-25 07:16:06,425 INFO] Loading test dataset from ../bert_data_test/cnndm.test.0.bert.pt, number of examples: 1\n",
            "[2019-09-25 07:16:33,972 INFO] Loading test dataset from ../bert_data_test/cnndm.test.0.bert.pt, number of examples: 1\n",
            "summary [` i have n't seen you in so long a time, thought i would stop and chat a while, ' said sly fox. ` i 'm having a good dinner tonight . wo n't you, mrs. rabbit, and your three little rabs come to dinner with me ? ' he asked. ` old sly fox has found our home and will destroy us, ' he said. The rabbit family were delighted with their new home in a woodland of towering pine needles, the ground covered in pine needles which made a soft carpeting. Mr. rabbit 's mother is having an engagement for today, but if you want us we can come tomorrow ', he told mr. rabbit . ` i ' . ' . i 'd rather . i '. I 'd . . . i . . 'd . 'd like him . ` . . 's . . ', ' . . he . . .      .   [unused4].]\n",
            "time used 25.024302005767822\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Cn03G9V15t3",
        "colab_type": "code",
        "outputId": "8cdcb3db-e6c7-430f-b4d5-6d94a01a619c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "#CNN_DM abstractive short\n",
        "%cd /content/PreSumm/src\n",
        "!python summarizer.py -task ext -mode test -test_from /content/PreSumm/models/CNN_DailyMail_Extractive/bertext_cnndm_transformer.pt -batch_size 32 -test_batch_size 500 -bert_data_path .../drive/My%20Drive/summary -log_file ../logs/val_abs_bert_cnndm -report_rouge False  -sep_optim true -use_interval true -visible_gpus 0,1,2 -max_pos 512 -max_src_nsents 100 -max_length 200 -alpha 0.95 -min_length 10 "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/PreSumm/src\n",
            "ext test\n",
            "[2019-09-25 07:21:23,927 INFO] Loading checkpoint from /content/PreSumm/models/CNN_DailyMail_Extractive/bertext_cnndm_transformer.pt\n",
            "Namespace(accum_count=1, alpha=0.95, batch_size=32, beam_size=5, bert_data_path='.../drive/My%20Drive/summary', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/val_abs_bert_cnndm', lower=True, lr=1, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=200, max_pos=512, max_src_nsents=100, max_src_ntokens_per_sent=200, max_tgt_len=140, max_tgt_ntokens=500, min_length=10, min_src_nsents=3, min_src_ntokens_per_sent=5, min_tgt_ntokens=5, mode='test', model_path='../../models/', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=False, result_path='../../results/cnndm', save_checkpoint_steps=5, seed=666, sep_optim=True, shard_size=2000, share_emb=False, task='ext', temp_dir='../../temp', test_all=False, test_batch_size=500, test_from='/content/PreSumm/models/CNN_DailyMail_Extractive/bertext_cnndm_transformer.pt', test_start_from=-1, train_from='', train_steps=1000, use_bert_basic_tokenizer=False, use_bert_emb=False, use_interval=True, visible_gpus='0,1,2', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=3)\n",
            "[2019-09-25 07:21:32,634 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
            "[2019-09-25 07:21:32,635 INFO] Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[2019-09-25 07:21:32,840 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "Traceback (most recent call last):\n",
            "  File \"summarizer.py\", line 144, in <module>\n",
            "    predictor = load_models_abs(args, device_id, cp, step)\n",
            "  File \"/content/PreSumm/src/train_abstractive.py\", line 215, in load_models_abs\n",
            "    model = AbsSummarizer(args, device, checkpoint)\n",
            "  File \"/content/PreSumm/src/models/model_builder.py\", line 210, in __init__\n",
            "    self.load_state_dict(checkpoint['model'], strict=True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 777, in load_state_dict\n",
            "    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
            "RuntimeError: Error(s) in loading state_dict for AbsSummarizer:\n",
            "\tMissing key(s) in state_dict: \"decoder.embeddings.weight\", \"decoder.pos_emb.pe\", \"decoder.transformer_layers.0.mask\", \"decoder.transformer_layers.0.self_attn.linear_keys.weight\", \"decoder.transformer_layers.0.self_attn.linear_keys.bias\", \"decoder.transformer_layers.0.self_attn.linear_values.weight\", \"decoder.transformer_layers.0.self_attn.linear_values.bias\", \"decoder.transformer_layers.0.self_attn.linear_query.weight\", \"decoder.transformer_layers.0.self_attn.linear_query.bias\", \"decoder.transformer_layers.0.self_attn.final_linear.weight\", \"decoder.transformer_layers.0.self_attn.final_linear.bias\", \"decoder.transformer_layers.0.context_attn.linear_keys.weight\", \"decoder.transformer_layers.0.context_attn.linear_keys.bias\", \"decoder.transformer_layers.0.context_attn.linear_values.weight\", \"decoder.transformer_layers.0.context_attn.linear_values.bias\", \"decoder.transformer_layers.0.context_attn.linear_query.weight\", \"decoder.transformer_layers.0.context_attn.linear_query.bias\", \"decoder.transformer_layers.0.context_attn.final_linear.weight\", \"decoder.transformer_layers.0.context_attn.final_linear.bias\", \"decoder.transformer_layers.0.feed_forward.w_1.weight\", \"decoder.transformer_layers.0.feed_forward.w_1.bias\", \"decoder.transformer_layers.0.feed_forward.w_2.weight\", \"decoder.transformer_layers.0.feed_forward.w_2.bias\", \"decoder.transformer_layers.0.feed_forward.layer_norm.weight\", \"decoder.transformer_layers.0.feed_forward.layer_norm.bias\", \"decoder.transformer_layers.0.layer_norm_1.weight\", \"decoder.transformer_layers.0.layer_norm_1.bias\", \"decoder.transformer_layers.0.layer_norm_2.weight\", \"decoder.transformer_layers.0.layer_norm_2.bias\", \"decoder.transformer_layers.1.mask\", \"decoder.transformer_layers.1.self_attn.linear_keys.weight\", \"decoder.transformer_layers.1.self_attn.linear_keys.bias\", \"decoder.transformer_layers.1.self_attn.linear_values.weight\", \"decoder.transformer_layers.1.self_attn.linear_values.bias\", \"decoder.transformer_layers.1.self_attn.linear_query.weight\", \"decoder.transformer_layers.1.self_attn.linear_query.bias\", \"decoder.transformer_layers.1.self_attn.final_linear.weight\", \"decoder.transformer_layers.1.self_attn.final_linear.bias\", \"decoder.transformer_layers.1.context_attn.linear_keys.weight\", \"decoder.transformer_layers.1.context_attn.linear_keys.bias\", \"decoder.transformer_layers.1.context_attn.linear_values.weight\", \"decoder.transformer_layers.1.context_attn.linear_values.bias\", \"decoder.transformer_layers.1.context_attn.linear_query.weight\", \"decoder.transformer_layers.1.context_attn.linear_query.bias\", \"decoder.transformer_layers.1.context_attn.final_linear.weight\", \"decoder.transformer_layers.1.context_attn.final_linear.bias\", \"decoder.transformer_layers.1.feed_forward.w_1.weight\", \"decoder.transformer_layers.1.feed_forward.w_1.bias\", \"decoder.transformer_layers.1.feed_forward.w_2.weight\", \"decoder.transformer_layers.1.feed_forward.w_2.bias\", \"decoder.transformer_layers.1.feed_forward.layer_norm.weight\", \"decoder.transformer_layers.1.feed_forward.layer_norm.bias\", \"decoder.transformer_layers.1.layer_norm_1.weight\", \"decoder.transformer_layers.1.layer_norm_1.bias\", \"decoder.transformer_layers.1.layer_norm_2.weight\", \"decoder.transformer_layers.1.layer_norm_2.bias\", \"decoder.transformer_layers.2.mask\", \"decoder.transformer_layers.2.self_attn.linear_keys.weight\", \"decoder.transformer_layers.2.self_attn.linear_keys.bias\", \"decoder.transformer_layers.2.self_attn.linear_values.weight\", \"decoder.transformer_layers.2.self_attn.linear_values.bias\", \"decoder.transformer_layers.2.self_attn.linear_query.weight\", \"decoder.transformer_layers.2.self_attn.linear_query.bias\", \"decoder.transformer_layers.2.self_attn.final_linear.weight\", \"decoder.transformer_layers.2.self_attn.final_linear.bias\", \"decoder.transformer_layers.2.context_attn.linear_keys.weight\", \"decoder.transformer_layers.2.context_attn.linear_keys.bias\", \"decoder.transformer_layers.2.context_attn.linear_values.weight\", \"decoder.transformer_layers.2.context_attn.linear_values.bias\", \"decoder.transformer_layers.2.context_attn.linear_query.weight\", \"decoder.transformer_layers.2.context_attn.linear_query.bias\", \"decoder.transformer_layers.2.context_attn.final_linear.weight\", \"decoder.transformer_layers.2.context_attn.final_linear.bias\", \"decoder.transformer_layers.2.feed_forward.w_1.weight\", \"decoder.transformer_layers.2.feed_forward.w_1.bias\", \"decoder.transformer_layers.2.feed_forward.w_2.weight\", \"decoder.transformer_layers.2.feed_forward.w_2.bias\", \"decoder.transformer_layers.2.feed_forward.layer_norm.weight\", \"decoder.transformer_layers.2.feed_forward.layer_norm.bias\", \"decoder.transformer_layers.2.layer_norm_1.weight\", \"decoder.transformer_layers.2.layer_norm_1.bias\", \"decoder.transformer_layers.2.layer_norm_2.weight\", \"decoder.transformer_layers.2.layer_norm_2.bias\", \"decoder.transformer_layers.3.mask\", \"decoder.transformer_layers.3.self_attn.linear_keys.weight\", \"decoder.transformer_layers.3.self_attn.linear_keys.bias\", \"decoder.transformer_layers.3.self_attn.linear_values.weight\", \"decoder.transformer_layers.3.self_attn.linear_values.bias\", \"decoder.transformer_layers.3.self_attn.linear_query.weight\", \"decoder.transformer_layers.3.self_attn.linear_query.bias\", \"decoder.transformer_layers.3.self_attn.final_linear.weight\", \"decoder.transformer_layers.3.self_attn.final_linear.bias\", \"decoder.transformer_layers.3.context_attn.linear_keys.weight\", \"decoder.transformer_layers.3.context_attn.linear_keys.bias\", \"decoder.transformer_layers.3.context_attn.linear_values.weight\", \"decoder.transformer_layers.3.context_attn.linear_values.bias\", \"decoder.transformer_layers.3.context_attn.linear_query.weight\", \"decoder.transformer_layers.3.context_attn.linear_query.bias\", \"decoder.transformer_layers.3.context_attn.final_linear.weight\", \"decoder.transformer_layers.3.context_attn.final_linear.bias\", \"decoder.transformer_layers.3.feed_forward.w_1.weight\", \"decoder.transformer_layers.3.feed_forward.w_1.bias\", \"decoder.transformer_layers.3.feed_forward.w_2.weight\", \"decoder.transformer_layers.3.feed_forward.w_2.bias\", \"decoder.transformer_layers.3.feed_forward.layer_norm.weight\", \"decoder.transformer_layers.3.feed_forward.layer_norm.bias\", \"decoder.transformer_layers.3.layer_norm_1.weight\", \"decoder.transformer_layers.3.layer_norm_1.bias\", \"decoder.transformer_layers.3.layer_norm_2.weight\", \"decoder.transformer_layers.3.layer_norm_2.bias\", \"decoder.transformer_layers.4.mask\", \"decoder.transformer_layers.4.self_attn.linear_keys.weight\", \"decoder.transformer_layers.4.self_attn.linear_keys.bias\", \"decoder.transformer_layers.4.self_attn.linear_values.weight\", \"decoder.transformer_layers.4.self_attn.linear_values.bias\", \"decoder.transformer_layers.4.self_attn.linear_query.weight\", \"decoder.transformer_layers.4.self_attn.linear_query.bias\", \"decoder.transformer_layers.4.self_attn.final_linear.weight\", \"decoder.transformer_layers.4.self_attn.final_linear.bias\", \"decoder.transformer_layers.4.context_attn.linear_keys.weight\", \"decoder.transformer_layers.4.context_attn.linear_keys.bias\", \"decoder.transformer_layers.4.context_attn.linear_values.weight\", \"decoder.transformer_layers.4.context_attn.linear_values.bias\", \"decoder.transformer_layers.4.context_attn.linear_query.weight\", \"decoder.transformer_layers.4.context_attn.linear_query.bias\", \"decoder.transformer_layers.4.context_attn.final_linear.weight\", \"decoder.transformer_layers.4.context_attn.final_linear.bias\", \"decoder.transformer_layers.4.feed_forward.w_1.weight\", \"decoder.transformer_layers.4.feed_forward.w_1.bias\", \"decoder.transformer_layers.4.feed_forward.w_2.weight\", \"decoder.transformer_layers.4.feed_forward.w_2.bias\", \"decoder.transformer_layers.4.feed_forward.layer_norm.weight\", \"decoder.transformer_layers.4.feed_forward.layer_norm.bias\", \"decoder.transformer_layers.4.layer_norm_1.weight\", \"decoder.transformer_layers.4.layer_norm_1.bias\", \"decoder.transformer_layers.4.layer_norm_2.weight\", \"decoder.transformer_layers.4.layer_norm_2.bias\", \"decoder.transformer_layers.5.mask\", \"decoder.transformer_layers.5.self_attn.linear_keys.weight\", \"decoder.transformer_layers.5.self_attn.linear_keys.bias\", \"decoder.transformer_layers.5.self_attn.linear_values.weight\", \"decoder.transformer_layers.5.self_attn.linear_values.bias\", \"decoder.transformer_layers.5.self_attn.linear_query.weight\", \"decoder.transformer_layers.5.self_attn.linear_query.bias\", \"decoder.transformer_layers.5.self_attn.final_linear.weight\", \"decoder.transformer_layers.5.self_attn.final_linear.bias\", \"decoder.transformer_layers.5.context_attn.linear_keys.weight\", \"decoder.transformer_layers.5.context_attn.linear_keys.bias\", \"decoder.transformer_layers.5.context_attn.linear_values.weight\", \"decoder.transformer_layers.5.context_attn.linear_values.bias\", \"decoder.transformer_layers.5.context_attn.linear_query.weight\", \"decoder.transformer_layers.5.context_attn.linear_query.bias\", \"decoder.transformer_layers.5.context_attn.final_linear.weight\", \"decoder.transformer_layers.5.context_attn.final_linear.bias\", \"decoder.transformer_layers.5.feed_forward.w_1.weight\", \"decoder.transformer_layers.5.feed_forward.w_1.bias\", \"decoder.transformer_layers.5.feed_forward.w_2.weight\", \"decoder.transformer_layers.5.feed_forward.w_2.bias\", \"decoder.transformer_layers.5.feed_forward.layer_norm.weight\", \"decoder.transformer_layers.5.feed_forward.layer_norm.bias\", \"decoder.transformer_layers.5.layer_norm_1.weight\", \"decoder.transformer_layers.5.layer_norm_1.bias\", \"decoder.transformer_layers.5.layer_norm_2.weight\", \"decoder.transformer_layers.5.layer_norm_2.bias\", \"decoder.layer_norm.weight\", \"decoder.layer_norm.bias\", \"generator.0.weight\", \"generator.0.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"ext_layer.pos_emb.pe\", \"ext_layer.transformer_inter.0.self_attn.linear_keys.weight\", \"ext_layer.transformer_inter.0.self_attn.linear_keys.bias\", \"ext_layer.transformer_inter.0.self_attn.linear_values.weight\", \"ext_layer.transformer_inter.0.self_attn.linear_values.bias\", \"ext_layer.transformer_inter.0.self_attn.linear_query.weight\", \"ext_layer.transformer_inter.0.self_attn.linear_query.bias\", \"ext_layer.transformer_inter.0.self_attn.final_linear.weight\", \"ext_layer.transformer_inter.0.self_attn.final_linear.bias\", \"ext_layer.transformer_inter.0.feed_forward.w_1.weight\", \"ext_layer.transformer_inter.0.feed_forward.w_1.bias\", \"ext_layer.transformer_inter.0.feed_forward.w_2.weight\", \"ext_layer.transformer_inter.0.feed_forward.w_2.bias\", \"ext_layer.transformer_inter.0.feed_forward.layer_norm.weight\", \"ext_layer.transformer_inter.0.feed_forward.layer_norm.bias\", \"ext_layer.transformer_inter.0.layer_norm.weight\", \"ext_layer.transformer_inter.0.layer_norm.bias\", \"ext_layer.transformer_inter.1.self_attn.linear_keys.weight\", \"ext_layer.transformer_inter.1.self_attn.linear_keys.bias\", \"ext_layer.transformer_inter.1.self_attn.linear_values.weight\", \"ext_layer.transformer_inter.1.self_attn.linear_values.bias\", \"ext_layer.transformer_inter.1.self_attn.linear_query.weight\", \"ext_layer.transformer_inter.1.self_attn.linear_query.bias\", \"ext_layer.transformer_inter.1.self_attn.final_linear.weight\", \"ext_layer.transformer_inter.1.self_attn.final_linear.bias\", \"ext_layer.transformer_inter.1.feed_forward.w_1.weight\", \"ext_layer.transformer_inter.1.feed_forward.w_1.bias\", \"ext_layer.transformer_inter.1.feed_forward.w_2.weight\", \"ext_layer.transformer_inter.1.feed_forward.w_2.bias\", \"ext_layer.transformer_inter.1.feed_forward.layer_norm.weight\", \"ext_layer.transformer_inter.1.feed_forward.layer_norm.bias\", \"ext_layer.transformer_inter.1.layer_norm.weight\", \"ext_layer.transformer_inter.1.layer_norm.bias\", \"ext_layer.layer_norm.weight\", \"ext_layer.layer_norm.bias\", \"ext_layer.wo.weight\", \"ext_layer.wo.bias\". \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfZNWOp8GLOH",
        "colab_type": "code",
        "outputId": "52972241-3869-4fa0-8a73-70ca7da6c334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#XSUM Abstractive (One Sntence)\n",
        "%cd /content/PreSumm/src\n",
        "!python summarizer.py -task abs -mode test -test_from /content/PreSumm/models/XSUM_OneSentence/model_step_30000.pt -batch_size 32 -test_batch_size 500 -bert_data_path ../bert_data/cnndm -log_file ../logs/val_abs_bert_cnndm -report_rouge False  -sep_optim true -use_interval true -visible_gpus 0,1,2 -max_pos 512 -max_src_nsents 100 -max_length 70 -alpha 0.95 -min_length 10 "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/PreSumm/src\n",
            "abs test\n",
            "[2019-09-25 07:19:15,104 INFO] Loading checkpoint from /content/PreSumm/models/XSUM_OneSentence/model_step_30000.pt\n",
            "Namespace(accum_count=1, alpha=0.95, batch_size=32, beam_size=5, bert_data_path='../bert_data/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0, 1, 2], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/val_abs_bert_cnndm', lower=True, lr=1, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=200, max_pos=512, max_src_nsents=100, max_src_ntokens_per_sent=200, max_tgt_len=140, max_tgt_ntokens=500, min_length=100, min_src_nsents=3, min_src_ntokens_per_sent=5, min_tgt_ntokens=5, mode='test', model_path='../../models/', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=False, result_path='../../results/cnndm', save_checkpoint_steps=5, seed=666, sep_optim=True, shard_size=2000, share_emb=False, task='abs', temp_dir='../../temp', test_all=False, test_batch_size=500, test_from='/content/PreSumm/models/XSUM_OneSentence/model_step_30000.pt', test_start_from=-1, train_from='', train_steps=1000, use_bert_basic_tokenizer=False, use_bert_emb=False, use_interval=True, visible_gpus='0,1,2', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=3)\n",
            "[2019-09-25 07:19:16,938 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
            "[2019-09-25 07:19:16,939 INFO] Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[2019-09-25 07:19:17,106 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "[2019-09-25 07:19:25,456 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "[2019-09-25 07:19:25,813 INFO] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "[2019-09-25 07:19:25,869 INFO] Processing                                 FOR CHILDREN:\n",
            "\n",
            "                                   Sly Fox\n",
            "\n",
            "    Mr. Rabbit sat on his front porch rocking, eating a great big carrot, \n",
            "and looking.\n",
            "\n",
            "    \"Looks like Sly Fox coming down the road,\" he said to himself, walking \n",
            "to the end of the porch. Shading his eyes with his paws, he exclaimed, \"It \n",
            "is Sly Fox.\" \n",
            "\n",
            "    \"Good morning Mr. Rabbit,\" cried Sly Fox, as he walked across the yard. \n",
            "\"Good morning,\" replied Mr. Rabbit, a slight frown on his face. \n",
            "\n",
            "    \"Well,\" said Sly Fox, \"as I haven't seen you in so long a time, thought \n",
            "I would stop and chat a while.\" \n",
            "\n",
            "    Mr. Rabbit could not be rude in his own home, even to an enemy, so he \n",
            "offered Sly Fox a seat on the porch. \n",
            "\n",
            "    \"Take a chair,\" he said politely. \tBut Sly Fox did not stay long, and as \n",
            "he was leaving, he asked: \"Mr. Rabbit, my mother is having a good dinner \n",
            "tonight. Won't you, Mrs. Rabbit, and your three little rabs come to dinner \n",
            "with me?\" \n",
            "\n",
            "    Oh, thought Mr. Rabbit, he knows about my little rabs and wants to take \n",
            "us off to eat us. He pretended to be disappointed as he replied: \"Sorry, Sly \n",
            "Fox, we have an engagement for today, but if you want us we can come \n",
            "tomorrow.\" \n",
            "\n",
            "    At this Sly Fox chuckled inwardly, and readily agreed to come for them \n",
            "the next day. Wishing Mr. Rabbit \"Good day\", he trotted on down the road \n",
            "toward his home. \n",
            "\n",
            "    As soon as he was out of sight, Mr. Rabbit ran into his house and called \n",
            "Mrs. Rabbit. \"Get all our things together,\" he said, \"and put rubber boots \n",
            "on our little rabs. We have to move quickly to the Piney Woods across the \n",
            "brook. Old Sly Fox has found our home and will destroy us.\" \n",
            "\n",
            "    In no time at all the Rabbit family had moved, and the little rabs were \n",
            "delighted with their new home. A woodland of towering pines it was, the \n",
            "ground covered with pine needles which made a soft carpeting. The wind made \n",
            "music in the pine trees, birds sang, and the fragrance of flowers filled the \n",
            "air. They found a huge hollow tree where Mr. Rabbit burrowed deep and made \n",
            "them a cozy home. Squirrels had left nuts hidden around in the old tree. \n",
            "Owls hooted throughout the night, crickets chirped merrily. \n",
            "\n",
            "    Next morning old Sly Fox knocked on the door where he had left Mr. \n",
            "Rabbit. Mrs. Hedgehog answered the door. \"Good morning, Mrs. Hedgehog. Is \n",
            "Mr. Rabbit in?\" inquired Sly Fox with a wicked grin and a cunning look in \n",
            "his eyes. \n",
            "\n",
            "    \"No,\" replied Mrs. Eedgehog, none too cordially. \"The Rabbit family \n",
            "moved to parts unknown right after you left yesterday.\" \n",
            "\n",
            "    \"Ah,\" exclaimed old Sly Fox, \"Mr. Rabbit and family were going to have \n",
            "dinner with me. My mother has planned a real feast. Why don't you come and \n",
            "enjoy it with us?\" \n",
            "\n",
            "    \"Oh,\" replied Mrs. Hedgehog, smacking her lips and thinking of all the \n",
            "goodies, \"I have just moved in and there is so much to do! Why not let it go \n",
            "until tomorrow?\" \n",
            "\n",
            "    \"Do you like nice young grasshoppers?\" asked Sly Fox softly. \n",
            "\n",
            "    \"Do I? Nothing so good as tender young grasshoppers,\" answered Mrs. \n",
            "Hedgehog, fairly dribbling at the mouth at the thought of such a dainty. \n",
            "\n",
            "    \"Well,\" said Sly Fox, \"we pass a field where there are any number of \n",
            "them. Come get in this sack, and when I stop in the field we will open the \n",
            "sack and rake in all of them we want. Mother will bake them with apples and \n",
            "they will be deilicious!\" This was too much for greedy Mrs. Hedgehog to \n",
            "resist, so in the sack she went. Sly Fox with a grin grabbed the sack, threw \n",
            "it over his shoulder and trotted toward home. \tAfter going a long way, Mrs. \n",
            "Hedgehog became suspicious and cried, \"How long before we reach that field \n",
            "of grasshoppers?\" \n",
            "\n",
            "    \"Why, you silly, greedy hedgehog, there is no field of grasshoppers for \n",
            "you. I am going to eat you for my dinner. It's you with apple dumplings that \n",
            "my mother will bake.\" \n",
            "\n",
            "    Every hair on Mrs. Hedgehog's head stood on end with fright. Oh, how \n",
            "foolish she had been! Her greed had trapped her. If only she had stayed home \n",
            "and straightened her house and cooked her own dinner, she would not have \n",
            "been in this sack to be eaten by Sly Fox. Greediness never pays, she thought \n",
            "to herself. \n",
            "\n",
            "    Sly Fox became tired, and as a slight rain had begun to fall, he looked \n",
            "for a dry place to sit down. Throwing the sack to the ground and chuckling \n",
            "at the thought of sitting on Mrs. Hedgehog, he dropped heavily upon the \n",
            "sack. \n",
            "\n",
            "    \"Wow, Wow!\" he cried, jumping quickly up, for Mrs. Hedgehog shot her \n",
            "sharp quills into him with all her might. \n",
            "\n",
            "    Sly Fox ran to and fro trying to pull out the quills, but they had gone \n",
            "too deep. Home he ran, screaming to his mother. Old Mother Fox threw him \n",
            "over a log and began pulling out the quills, at the same time calling to a \n",
            "neighbor fox to bring some honey to put on the places where the quills had \n",
            "been. \n",
            "\n",
            "    Mrs. Hedgehog crawled out of the bag and began walking slowly toward \n",
            "home. She thought to herself that never again would she be so greedy and \n",
            "allow herself to be fooled by Sly Fox or any one else. \n",
            "\n",
            "    Meanwhile, Mr. Rabbit and family were living happily in Piney Woods. The \n",
            "little rabs played on the crystal clear brook that ran through the woods, \n",
            "wading, sailing little leaf boats, and trying to catch the silvery minnows \n",
            "darting here and there. \n",
            "\n",
            "    Late one evening Papa and Mama Rabbit were sitting before the cozy fire \n",
            "talking. Papa Rabbit had on his house robe and bedroom slippers, reading the \n",
            "newspaper. Every now and then he looked over his spectacles lovingly at \n",
            "dainty little Mama Rabbit, dressed in a flowered housecoat and red slippers \n",
            "and knitting little socks for the little rabs. \n",
            "\n",
            "    \"Sniff! Sniff! Sniff!\" came suddenly to their ears. \n",
            "\n",
            "    \"Sly Fox!\" whispered Papa Rabbit, his face now full of concern and \n",
            "alarm. \n",
            "\n",
            "    \"Yes,\" agreed Mama Rabbit, her voice trembling with fright. \n",
            "\n",
            "    \"Go cover the little rabs with straw and tell them to be very, very \n",
            "quiet,\" instructed Papa Rabbit. \n",
            "\n",
            "    Mrs. Rabbit quickly covered the little rabs and cautioned them to be as \n",
            "quiet as mice. Since they were well behaved and obedient little rabs, they \n",
            "did just as their mother told them. \n",
            "\n",
            "    \"I left my big stick beside the old oak tree,\" cried Papa Rabbit under \n",
            "his breath. \"What shall we do?\" \n",
            "\n",
            "    \"Sniff! Sniff! Sniff!\" went Sly Fox again, scratching up the earth by \n",
            "the old hollow tree as he began to dig furiously. The poor little Rabbit \n",
            "family sat still and frightened, their hearts thumping, their paws shaking, \n",
            "and their eyes bulging with panic. Suddenly in the distance they heard the \n",
            "\"Toot! Toot!Toot!\" of horns, and the \"Woof! Woof! Woof!\" of barking dogs. \n",
            "\n",
            "    Papa Rabbit whispered, \"Fox hunters!\" as his heart gave a bound of \n",
            "relief. \n",
            "\n",
            "    Nearer and nearer came the baying of the hounds and the music of the \n",
            "horns. Old Sly Fox was so busily digging that he failed to hear at first, \n",
            "but suddenly he stopped digging, and threw back his ears to listen. Then he \n",
            "quickly jumped away from the log where the Rabbit family lived and started \n",
            "running. \n",
            "\n",
            "    But the hounds were right after him, baying loudly with all their might. \n",
            "The horses' feet beat out an excited rhythm as the red-coated fox hunters \n",
            "urged them on in the chase. Up hill, over the meadows they ran. \n",
            "\n",
            "    Sly Fox was now running for his life, but the dogs were getting closer \n",
            "and closer. He jumped across the brook and spied a hole among some bushes. \n",
            "Into this he slid, and as the dogs went down the side of the stream of water \n",
            "before they jumped across they lost his scent. Sly Fox quickly ran out of \n",
            "the hole and took off in the opposite direction from the way the dogs were \n",
            "going. He had been so frightened and so near death that he resolved to \n",
            "himself never to bother the Rabbit family again. \n",
            "\n",
            "    Meanwhile, when Papa Rabbit had heard the hounds start the chase, he \n",
            "turned to Mama Rabbit and cried, \"Safe at last! Call our little rabs for \n",
            "prayers of thanksgiving and praise to our Father which art in heaven.\" \n",
            "\n",
            "    After prayers, Mama Rabbit hustled about making mint tea for her and \n",
            "Papa Rabbit, and hot chocolate piled high with whipped cream for the little \n",
            "rabs. After that time they Lived happily among the great whispering pines, \n",
            "never bothered by old Sly Fox.\n",
            "                                                          --Beulah Murrelle\n",
            "[2019-09-25 07:19:25,914 INFO] Saving to ../bert_data_test/cnndm.test.0.bert.pt\n",
            "[2019-09-25 07:19:25,919 INFO] Loading test dataset from ../bert_data_test/cnndm.test.0.bert.pt, number of examples: 1\n",
            "[2019-09-25 07:19:28,796 INFO] Loading test dataset from ../bert_data_test/cnndm.test.0.bert.pt, number of examples: 1\n",
            "summary [It 's almost a year since your family 's new home was built in the woods in north-east of england, but what does n't mean for your family to eat your children ? 's picture of your favourite pet dogs in your home, and what 's your family have to do with their children in your garden ? 's homes ? 's home ? . . . 's woods . 's shimmy affairs . 's shirazzis shirking ..]\n",
            "time used 2.7452502250671387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ELmEI2-A29i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_path = '/content/run_{:%Y%m%d_%H%M%S}.zip'.format(datetime.utcnow())\n",
        "!zip -r zip_path /content/PreSumm/results\n",
        "#from google.colab import files\n",
        "#files.download(\"/content/results.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}